{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/manojach87/IWildCam2020/blob/master/src/prepData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 0,
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ttks-5ou4N_4"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 0,
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCrKCx24j81"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def readJSONFile(path):\n",
    "    import json\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 0,
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8wQm94m4ufZ"
   },
   "outputs": [],
   "source": [
    "def getImageAsArray(path,df):\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    dataF=[{\"file_name\":file_name,\n",
    "              \"image\":\n",
    "                  np.concatenate(\n",
    "                  (np.asarray(Image.open(os.path.join(path, file_name)).convert('L'))) #.tolist()\n",
    "                  ).ravel().tolist()\n",
    "              } \n",
    "             for file_name in df.file_name[:]]\n",
    "    return dataF"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 0,
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPvf0aQU4ynU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 0,
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRBDTFX3rxoX"
   },
   "outputs": [],
   "source": [
    "def getTrainData(jsonFilePath):\n",
    "    global categories\n",
    "    global trainDf\n",
    "    global trainDf1\n",
    "    data = readJson.readJSONFile(jsonFilePath)\n",
    "\n",
    "    annotations = data[\"annotations\"]\n",
    "    images=data[\"images\"]\n",
    "    categories = data[\"categories\"]\n",
    "    info = data[\"info\"]\n",
    "\n",
    "    # Convert to Data frame\n",
    "\n",
    "    annotations = pd.DataFrame.from_dict(annotations)\n",
    "    images = pd.DataFrame.from_dict(images)\n",
    "    categories = pd.DataFrame.from_dict(categories)\n",
    "\n",
    "    #Remove data from memory\n",
    "    del data\n",
    "\n",
    "    #Create column image_id to use for merging the two data frames\n",
    "    images[\"image_id\"]  = images[\"id\"]\n",
    "\n",
    "    # Merge annotations and images on image_id\n",
    "\n",
    "    trainDf1 = (pd.merge(annotations, images, on='image_id'))\n",
    "    #Remove Unnecessary fields\n",
    "    trainDf1.drop([\"id_y\",\"id_x\"], axis = 1, inplace=True)\n",
    "\n",
    "    #print(trainDf1.columns)\n",
    "    \n",
    "    # Unset annotations and images dataframe as they are no longer needed\n",
    "    del annotations\n",
    "    del images\n",
    "\n",
    "    # Open the image from working directory\n",
    "\n",
    "    trainDf=getImageAsArray(trainPath,trainDf1)\n",
    "\n",
    "    #Convert to dataframe\n",
    "    \n",
    "    trainDf = pd.DataFrame.from_dict(trainDf)\n",
    "\n",
    "\n",
    "    trainDf1.drop(['count', 'image_id', 'seq_id', 'width', 'height', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
    "\n",
    "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
    "    trainDf = (pd.merge(trainDf, trainDf1, on='file_name'))\n",
    "    \n",
    "    #del trainDf1\n",
    "\n",
    "    #trainDf=trainDf.drop(['file_name', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1)\n",
    "    #trainDf.drop(['seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
    "\n",
    "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
    "    trainDf=np.asarray(trainDf)\n",
    "\n",
    "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
    "    trainDf=[[arr[0]]+[arr[2]]+list(arr[1]) for arr in trainDf]\n",
    "    \n",
    "    # Converting back to dataframe\n",
    "    trainDf=pd.DataFrame(trainDf)\n",
    "    \n",
    "    #Write the processed file to csv file for future use\n",
    "    #trainDf.to_csv(\"trainDf.csv\")\n",
    "    \n",
    "    \n",
    "    categoriesFromTrainData=trainDf[1].unique()\n",
    "\n",
    "    categoriesFromTrainData=pd.DataFrame(categoriesFromTrainData)\n",
    "    categoriesFromTrainData.rename(columns={0: \"id\"},inplace = True)\n",
    "    categoriesFromTrainData.sort_values(\"id\", axis = 0, ascending = True, \n",
    "                  inplace = True, na_position ='last')\n",
    "    \n",
    "    categoriesFromTrainData[\"Sk\"]=[i for i in range(0,len(categoriesFromTrainData))]\n",
    "    \n",
    "    categories = (pd.merge(categoriesFromTrainData, categories, on='id'))\n",
    "    trainDf.rename(columns={0: \"file_name\",1:\"id\"},inplace = True)\n",
    "    trainDf = (pd.merge(trainDf, categories, on='id'))\n",
    "    trainDf[\"id\"]=trainDf[\"Sk\"]\n",
    "    trainDf.drop(columns=[\"Sk\",\"count\",\"name\",\"file_name\"],axis=1, inplace = True)\n",
    "    trainDf.rename(columns={\"id\": \"Sk\"},inplace = True)\n",
    "    \n",
    "    #return categories, trainDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acP8laavr_Ua"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def getTestData(jsonFilePath):\n",
    "    data = readJson.readJSONFile(jsonTestFilePath)\n",
    "    \n",
    "    images=data[\"images\"]\n",
    "    categories = data[\"categories\"]\n",
    "    info = data[\"info\"]\n",
    "    \n",
    "    # Convert to Data frame\n",
    "    \n",
    "    images = pd.DataFrame.from_dict(images)\n",
    "    categories = pd.DataFrame.from_dict(categories)\n",
    "    \n",
    "    #Remove data from memory\n",
    "    del data, info\n",
    "    \n",
    "    # Remove Unnecessary fields from images\n",
    "    testDf1 = pd.DataFrame(images.file_name)\n",
    "    \n",
    "    # Unset images dataframe as it is no longer needed\n",
    "    #del images\n",
    "    \n",
    "    # Open the image from working directory\n",
    "    \n",
    "    testDf=getImageAsArray(testPath,testDf1)\n",
    "    \n",
    "    #Convert to dataframe\n",
    "    \n",
    "    testDf = pd.DataFrame.from_dict(testDf)\n",
    "    \n",
    "    \n",
    "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
    "    testDf = (pd.merge(testDf, testDf1, on='file_name'))\n",
    "    \n",
    "    testFiles=testDf[\"file_name\"]\n",
    "    testDf.drop(['file_name'],axis=1, inplace=True)\n",
    "\n",
    "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
    "    testDf=np.asarray(testDf)\n",
    "    \n",
    "    \n",
    "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
    "    testDf=[list(arr[0]) for arr in testDf]\n",
    "    \n",
    "    # Converting back to dataframe\n",
    "    testDf=pd.DataFrame(testDf)\n",
    "    \n",
    "    #Write the processed file to csv file for future use\n",
    "    #testDf.to_csv(\"testDf.100X100.csv\")\n",
    "    \n",
    "    return testFiles, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "5xodmYIk5N7V",
    "outputId": "42ab8afa-e922-48be-daef-613ad3f7a4b2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-24bca255b2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetTrainData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonTrainFilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonTrainFilePath' is not defined"
     ]
    }
   ],
   "source": [
    "#categories, trainDf=getTrainData(jsonTrainFilePath)\n",
    "getTrainData(jsonTrainFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "vGfo9nwc5Usq",
    "outputId": "f4ffe6ed-b65e-467a-8eed-379c4aa5ff0e"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PA-hf5um6djC",
    "outputId": "4bd8fd34-8c6a-4595-d187-07043e2afdba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#[x[0] for x in os.walk(\"/drive\")]\n",
    "next(os.walk('./drive/My Drive/drive.1/iWildCam'))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__GOZhnO7cXv"
   },
   "outputs": [],
   "source": [
    "root='./drive/My Drive/drive.1/iWildCam/'\n",
    "jsonTrainFilePath=root+'iwildcam2020_train_annotations.json'\n",
    "jsonTestFilePath=root+'iwildcam2020_test_information.json'\n",
    "trainPath=root+'train\\\\28X28\\\\'\n",
    "testPath =root+'test\\\\100X100\\\\'\n",
    "trainDfZipFile=root+'train.zip'\n",
    "trainDfFile=root+'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v24mx2kH7trw"
   },
   "outputs": [],
   "source": [
    "def unzip(path_to_zip_file,directory_to_extract_to):\n",
    "  import zipfile\n",
    "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "      zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "  zip = ZipFile('file.zip')\n",
    "  zip.extractall()\n",
    "  \n",
    "from zipfile import ZipFile\n",
    "import subprocess, sys\n",
    "\n",
    "def Unzip(zipFile, destinationDirectory):\n",
    "    try:\n",
    "        with ZipFile(zipFile, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in different directory\n",
    "            zipObj.extractall(destinationDirectory)\n",
    "    except:\n",
    "        print(\"An exception occurred extracting with Python ZipFile library.\")\n",
    "        print(\"Attempting to extract using 7zip\")\n",
    "        subprocess.Popen([\"7z\", \"e\", f\"{zipFile}\", f\"-o{destinationDirectory}\", \"-y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-x3s-xf28Kf4",
    "outputId": "631cb534-1b53-4e7d-f411-cb02ce6f8b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred extracting with Python ZipFile library.\n",
      "Attempting to extract using 7zip\n"
     ]
    }
   ],
   "source": [
    "Unzip(trainDfZipFile,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0_7__f87shv"
   },
   "outputs": [],
   "source": [
    "#trainDf.to_csv(\"train.csv\")\n",
    "#trainDf=pd.read_csv(\"train.csv\")\n",
    "trainDf=pd.read_csv(trainDfFile)\n",
    "\n",
    "#.to_csv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKt1HDXWBGWD"
   },
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"0\"]), trainDf[\"0\"], test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"Sk\"]), trainDf[\"Sk\"], test_size=0.1)\n",
    "#%%\n",
    "#Skipping Test train Split\n",
    "#X_train=trainDf.drop(columns=[\"Sk\"])\n",
    "y_train=trainDf[\"Sk\"]\n",
    "X_train=trainDf.drop(columns=[\"Sk\",\"Unnamed: 0\"])\n",
    "#%%\n",
    "#del trainDf\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clw-ZD2IsAZ0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_train = X_train/255.0\n",
    "#X_test  = X_test /255.0\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "X_train.replace(0,1e-10, inplace=True)\n",
    "#%%\n",
    "X_test.replace(0,1e-10, inplace=True)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "X_train = X_train.values.reshape(-1,100,100,1)\n",
    "#X_test = X_test.values.reshape(-1,100,100,1)\n",
    "# In[5]:\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "X_train = np.asarray(X_train)\n",
    "# In[6]:\n",
    "\n",
    "print(X_train[np.isnan(X_train)])\n",
    "print(X_test [np.isnan(X_test )])\n",
    "print(y_train[np.isnan(y_train)])\n",
    "print(y_test [np.isnan(y_test )])\n",
    "\n",
    "#%%\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "#%%\n",
    "#model=tf.keras.models.load_model(\"model.1.save\")\n",
    "#%%\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (100,100,1)))\n",
    "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "# model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "# model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(keras.layers.Dense(267, activation = \"softmax\"))\n",
    "model.add(keras.layers.Dense(len(categories), activation = \"softmax\"))\n",
    "\n",
    "#model.add(keras.layers.Dense(len(categories), activation = \"softmax\", weights = [np.zeros([[100,100,1], len(categories)]), np.zeros(len(categories))]))\n",
    "\n",
    "#optimizer = keras.optimizers.RMSprop(learning_rate=0.00, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#optimizer = keras.optimizers.SGD()\n",
    "#optimizer = keras.optimizers.RMSprop()\n",
    "\n",
    "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "model.load_weights(filepath=root+'final_weight.h5')\n",
    "#%%\n",
    "model.fit(X_train, y_train, epochs=8)\n",
    "#%%\n",
    "model.save_weights(filepath='final_weight.conv2x3.1024.h5')\n",
    "#%%\n",
    "testFiles, testDf =getTestData(jsonTestFilePath)\n",
    "\n",
    "#%%\n",
    "testDf.to_csv(\"testDf.csv\")\n",
    "#%%\n",
    "pd.DataFrame(testFiles).to_csv(\"testFiles.csv\")\n",
    "# In[2]:\n",
    "# Normalize the values\n",
    "testDf = testDf /255.0\n",
    "\n",
    "#%%\n",
    "\n",
    "X_train.replace(0,1e-10, inplace=True)\n",
    "\n",
    "#%%\n",
    "\n",
    "#testDf = testDf.values.reshape(-1,100,100,1)\n",
    "\n",
    "#%%\n",
    "# Replace very small number with zero  \n",
    "testDf[testDf == 0] = 1e-10\n",
    "\n",
    "#%%\n",
    "preds = model.predict(testDf)\n",
    "#%%\n",
    "#%%\n",
    "preds = pd.DataFrame(data=preds, columns=range(0,len(categories)))\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "preds = preds.idxmax(axis=1)\n",
    "\n",
    "#%%\n",
    "preds=pd.DataFrame(preds)\n",
    "#%%\n",
    "\n",
    "preds['index1'] = preds.index\n",
    "\n",
    "#%%\n",
    "\n",
    "#predList[\"Sk\"]=predList[0]\n",
    "preds.rename(columns={0: \"Sk\"},inplace = True)\n",
    "#%%\n",
    "#predList=predList.drop(columns=[0],axis=1)\n",
    "#%%\n",
    "\n",
    "preds = (pd.merge(preds, categories, on=\"Sk\"))\n",
    "\n",
    "#%%\n",
    "#.drop(columns=[\"\",\"\"])\n",
    "#%%\n",
    "#del X_train\n",
    "\n",
    "#%%\n",
    "#predList[\"Category\"]=predList[\"id\"]\n",
    "preds.rename(columns={\"id\": \"Category\"},inplace = True)\n",
    "\n",
    "#%%\n",
    "preds.drop(columns=[\"Sk\",\"count\",\"name\"], axis=1, inplace = True)\n",
    "#%%\n",
    "preds.sort_values(\"index1\", axis = 0, ascending = True, inplace = True, na_position ='last')\n",
    "#%%\n",
    "#predList=predList.drop(columns=[\"0_x\",\"0_y\"], axis=1)\n",
    "#%%\n",
    "##predList = (pd.merge(predList, , on=\"Sk\"))\n",
    "preds[\"file_name\"]=testFiles\n",
    "#%%\n",
    "data = readJson.readJSONFile(jsonTestFilePath)\n",
    "    \n",
    "images=data[\"images\"]\n",
    "\n",
    "images = pd.DataFrame.from_dict(images)\n",
    "#%%\n",
    "images = images[[\"id\",\"file_name\"]]\n",
    "# categories = pd.DataFrame.from_dict(categories)\n",
    "#%%\n",
    "preds=(pd.merge(preds, images, on=\"file_name\"))[[\"id\",\"Category\"]]\n",
    "\n",
    "#%%\n",
    "submission=pd.read_csv(\"../data/iwildcam-2020/sample_submission.csv\").drop(columns=[\"Category\"],axis=1)\n",
    "#%%\n",
    "preds.rename(columns={\"id\": \"Id\"},inplace = True)\n",
    "\n",
    "#%%\n",
    "preds=(pd.merge(submission, preds, on=\"Id\")) #[[\"Id\",\"Category\"]]\n",
    "\n",
    "#%%\n",
    "preds.to_csv(\"submissison.20200321.2.csv\",index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "prepData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.7"
=======
   "version": "3.6.10"
>>>>>>> d83ee193f557357aa01ec6d4338eb6319cf212b0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
