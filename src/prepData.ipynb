{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "prepData.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manojach87/IWildCam2020/blob/master/src/prepData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttks-5ou4N_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rCrKCx24j81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "def readJSONFile(path):\n",
        "    import json\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8wQm94m4ufZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getImageAsArray(path,df):\n",
        "    from PIL import Image\n",
        "    import os\n",
        "    dataF=[{\"file_name\":file_name,\n",
        "              \"image\":\n",
        "                  np.concatenate(\n",
        "                  (np.asarray(Image.open(os.path.join(path, file_name)).convert('L'))) #.tolist()\n",
        "                  ).ravel().tolist()\n",
        "              } \n",
        "             for file_name in df.file_name[:]]\n",
        "    return dataF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPvf0aQU4ynU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRBDTFX3rxoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTrainData(jsonFilePath):\n",
        "    global categories\n",
        "    global trainDf\n",
        "    global trainDf1\n",
        "    data = readJson.readJSONFile(jsonFilePath)\n",
        "\n",
        "    annotations = data[\"annotations\"]\n",
        "    images=data[\"images\"]\n",
        "    categories = data[\"categories\"]\n",
        "    info = data[\"info\"]\n",
        "\n",
        "    # Convert to Data frame\n",
        "\n",
        "    annotations = pd.DataFrame.from_dict(annotations)\n",
        "    images = pd.DataFrame.from_dict(images)\n",
        "    categories = pd.DataFrame.from_dict(categories)\n",
        "\n",
        "    #Remove data from memory\n",
        "    del data\n",
        "\n",
        "    #Create column image_id to use for merging the two data frames\n",
        "    images[\"image_id\"]  = images[\"id\"]\n",
        "\n",
        "    # Merge annotations and images on image_id\n",
        "\n",
        "    trainDf1 = (pd.merge(annotations, images, on='image_id'))\n",
        "    #Remove Unnecessary fields\n",
        "    trainDf1.drop([\"id_y\",\"id_x\"], axis = 1, inplace=True)\n",
        "\n",
        "    #print(trainDf1.columns)\n",
        "    \n",
        "    # Unset annotations and images dataframe as they are no longer needed\n",
        "    del annotations\n",
        "    del images\n",
        "\n",
        "    # Open the image from working directory\n",
        "\n",
        "    trainDf=getImageAsArray(trainPath,trainDf1)\n",
        "\n",
        "    #Convert to dataframe\n",
        "    \n",
        "    trainDf = pd.DataFrame.from_dict(trainDf)\n",
        "\n",
        "\n",
        "    trainDf1.drop(['count', 'image_id', 'seq_id', 'width', 'height', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
        "\n",
        "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
        "    trainDf = (pd.merge(trainDf, trainDf1, on='file_name'))\n",
        "    \n",
        "    #del trainDf1\n",
        "\n",
        "    #trainDf=trainDf.drop(['file_name', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1)\n",
        "    #trainDf.drop(['seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
        "\n",
        "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
        "    trainDf=np.asarray(trainDf)\n",
        "\n",
        "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
        "    trainDf=[[arr[0]]+[arr[2]]+list(arr[1]) for arr in trainDf]\n",
        "    \n",
        "    # Converting back to dataframe\n",
        "    trainDf=pd.DataFrame(trainDf)\n",
        "    \n",
        "    #Write the processed file to csv file for future use\n",
        "    #trainDf.to_csv(\"trainDf.csv\")\n",
        "    \n",
        "    \n",
        "    categoriesFromTrainData=trainDf[1].unique()\n",
        "\n",
        "    categoriesFromTrainData=pd.DataFrame(categoriesFromTrainData)\n",
        "    categoriesFromTrainData.rename(columns={0: \"id\"},inplace = True)\n",
        "    categoriesFromTrainData.sort_values(\"id\", axis = 0, ascending = True, \n",
        "                  inplace = True, na_position ='last')\n",
        "    \n",
        "    categoriesFromTrainData[\"Sk\"]=[i for i in range(0,len(categoriesFromTrainData))]\n",
        "    \n",
        "    categories = (pd.merge(categoriesFromTrainData, categories, on='id'))\n",
        "    trainDf.rename(columns={0: \"file_name\",1:\"id\"},inplace = True)\n",
        "    trainDf = (pd.merge(trainDf, categories, on='id'))\n",
        "    trainDf[\"id\"]=trainDf[\"Sk\"]\n",
        "    trainDf.drop(columns=[\"Sk\",\"count\",\"name\",\"file_name\"],axis=1, inplace = True)\n",
        "    trainDf.rename(columns={\"id\": \"Sk\"},inplace = True)\n",
        "    \n",
        "    #return categories, trainDf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acP8laavr_Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "def getTestData(jsonFilePath):\n",
        "    data = readJson.readJSONFile(jsonTestFilePath)\n",
        "    \n",
        "    images=data[\"images\"]\n",
        "    categories = data[\"categories\"]\n",
        "    info = data[\"info\"]\n",
        "    \n",
        "    # Convert to Data frame\n",
        "    \n",
        "    images = pd.DataFrame.from_dict(images)\n",
        "    categories = pd.DataFrame.from_dict(categories)\n",
        "    \n",
        "    #Remove data from memory\n",
        "    del data, info\n",
        "    \n",
        "    # Remove Unnecessary fields from images\n",
        "    testDf1 = pd.DataFrame(images.file_name)\n",
        "    \n",
        "    # Unset images dataframe as it is no longer needed\n",
        "    #del images\n",
        "    \n",
        "    # Open the image from working directory\n",
        "    \n",
        "    testDf=getImageAsArray(testPath,testDf1)\n",
        "    \n",
        "    #Convert to dataframe\n",
        "    \n",
        "    testDf = pd.DataFrame.from_dict(testDf)\n",
        "    \n",
        "    \n",
        "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
        "    testDf = (pd.merge(testDf, testDf1, on='file_name'))\n",
        "    \n",
        "    testFiles=testDf[\"file_name\"]\n",
        "    testDf.drop(['file_name'],axis=1, inplace=True)\n",
        "\n",
        "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
        "    testDf=np.asarray(testDf)\n",
        "    \n",
        "    \n",
        "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
        "    testDf=[list(arr[0]) for arr in testDf]\n",
        "    \n",
        "    # Converting back to dataframe\n",
        "    testDf=pd.DataFrame(testDf)\n",
        "    \n",
        "    #Write the processed file to csv file for future use\n",
        "    #testDf.to_csv(\"testDf.100X100.csv\")\n",
        "    \n",
        "    return testFiles, testDf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xodmYIk5N7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "42ab8afa-e922-48be-daef-613ad3f7a4b2"
      },
      "source": [
        "#categories, trainDf=getTrainData(jsonTrainFilePath)\n",
        "getTrainData(jsonTrainFilePath)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-24bca255b2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetTrainData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonTrainFilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'jsonTrainFilePath' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGfo9nwc5Usq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f4ffe6ed-b65e-467a-8eed-379c4aa5ff0e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA-hf5um6djC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bd8fd34-8c6a-4595-d187-07043e2afdba"
      },
      "source": [
        "import os\n",
        "\n",
        "#[x[0] for x in os.walk(\"/drive\")]\n",
        "next(os.walk('./drive/My Drive/drive.1/iWildCam'))[1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__GOZhnO7cXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root='./drive/My Drive/drive.1/iWildCam/'\n",
        "jsonTrainFilePath=root+'iwildcam2020_train_annotations.json'\n",
        "jsonTestFilePath=root+'iwildcam2020_test_information.json'\n",
        "trainPath=root+'train\\\\28X28\\\\'\n",
        "testPath =root+'test\\\\100X100\\\\'\n",
        "trainDfZipFile=root+'train.zip'\n",
        "trainDfFile=root+'train.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v24mx2kH7trw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unzip(path_to_zip_file,directory_to_extract_to):\n",
        "  import zipfile\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "  zip = ZipFile('file.zip')\n",
        "  zip.extractall()\n",
        "  \n",
        "from zipfile import ZipFile\n",
        "import subprocess, sys\n",
        "\n",
        "def Unzip(zipFile, destinationDirectory):\n",
        "    try:\n",
        "        with ZipFile(zipFile, 'r') as zipObj:\n",
        "            # Extract all the contents of zip file in different directory\n",
        "            zipObj.extractall(destinationDirectory)\n",
        "    except:\n",
        "        print(\"An exception occurred extracting with Python ZipFile library.\")\n",
        "        print(\"Attempting to extract using 7zip\")\n",
        "        subprocess.Popen([\"7z\", \"e\", f\"{zipFile}\", f\"-o{destinationDirectory}\", \"-y\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x3s-xf28Kf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "631cb534-1b53-4e7d-f411-cb02ce6f8b48"
      },
      "source": [
        "Unzip(trainDfZipFile,root)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An exception occurred extracting with Python ZipFile library.\n",
            "Attempting to extract using 7zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0_7__f87shv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainDf.to_csv(\"train.csv\")\n",
        "#trainDf=pd.read_csv(\"train.csv\")\n",
        "trainDf=pd.read_csv(trainDfFile)\n",
        "\n",
        "#.to_csv(\"train.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKt1HDXWBGWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In[1]:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"0\"]), trainDf[\"0\"], test_size=0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"Sk\"]), trainDf[\"Sk\"], test_size=0.1)\n",
        "#%%\n",
        "#Skipping Test train Split\n",
        "#X_train=trainDf.drop(columns=[\"Sk\"])\n",
        "y_train=trainDf[\"Sk\"]\n",
        "X_train=trainDf.drop(columns=[\"Sk\",\"Unnamed: 0\"])\n",
        "#%%\n",
        "#del trainDf\n",
        "# In[2]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clw-ZD2IsAZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X_train = X_train/255.0\n",
        "#X_test  = X_test /255.0\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "X_train.replace(0,1e-10, inplace=True)\n",
        "#%%\n",
        "X_test.replace(0,1e-10, inplace=True)\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "X_train = X_train.values.reshape(-1,100,100,1)\n",
        "#X_test = X_test.values.reshape(-1,100,100,1)\n",
        "# In[5]:\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "X_train = np.asarray(X_train)\n",
        "# In[6]:\n",
        "\n",
        "print(X_train[np.isnan(X_train)])\n",
        "print(X_test [np.isnan(X_test )])\n",
        "print(y_train[np.isnan(y_train)])\n",
        "print(y_test [np.isnan(y_test )])\n",
        "\n",
        "#%%\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "#%%\n",
        "#model=tf.keras.models.load_model(\"model.1.save\")\n",
        "#%%\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (100,100,1)))\n",
        "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "# model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "# model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(1024, activation = \"relu\"))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "#model.add(keras.layers.Dense(267, activation = \"softmax\"))\n",
        "model.add(keras.layers.Dense(len(categories), activation = \"softmax\"))\n",
        "\n",
        "#model.add(keras.layers.Dense(len(categories), activation = \"softmax\", weights = [np.zeros([[100,100,1], len(categories)]), np.zeros(len(categories))]))\n",
        "\n",
        "#optimizer = keras.optimizers.RMSprop(learning_rate=0.00, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "#optimizer = keras.optimizers.SGD()\n",
        "#optimizer = keras.optimizers.RMSprop()\n",
        "\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "model.load_weights(filepath=root+'final_weight.h5')\n",
        "#%%\n",
        "model.fit(X_train, y_train, epochs=8)\n",
        "#%%\n",
        "model.save_weights(filepath='final_weight.conv2x3.1024.h5')\n",
        "#%%\n",
        "testFiles, testDf =getTestData(jsonTestFilePath)\n",
        "\n",
        "#%%\n",
        "testDf.to_csv(\"testDf.csv\")\n",
        "#%%\n",
        "pd.DataFrame(testFiles).to_csv(\"testFiles.csv\")\n",
        "# In[2]:\n",
        "# Normalize the values\n",
        "testDf = testDf /255.0\n",
        "\n",
        "#%%\n",
        "\n",
        "X_train.replace(0,1e-10, inplace=True)\n",
        "\n",
        "#%%\n",
        "\n",
        "#testDf = testDf.values.reshape(-1,100,100,1)\n",
        "\n",
        "#%%\n",
        "# Replace very small number with zero  \n",
        "testDf[testDf == 0] = 1e-10\n",
        "\n",
        "#%%\n",
        "preds = model.predict(testDf)\n",
        "#%%\n",
        "#%%\n",
        "preds = pd.DataFrame(data=preds, columns=range(0,len(categories)))\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "preds = preds.idxmax(axis=1)\n",
        "\n",
        "#%%\n",
        "preds=pd.DataFrame(preds)\n",
        "#%%\n",
        "\n",
        "preds['index1'] = preds.index\n",
        "\n",
        "#%%\n",
        "\n",
        "#predList[\"Sk\"]=predList[0]\n",
        "preds.rename(columns={0: \"Sk\"},inplace = True)\n",
        "#%%\n",
        "#predList=predList.drop(columns=[0],axis=1)\n",
        "#%%\n",
        "\n",
        "preds = (pd.merge(preds, categories, on=\"Sk\"))\n",
        "\n",
        "#%%\n",
        "#.drop(columns=[\"\",\"\"])\n",
        "#%%\n",
        "#del X_train\n",
        "\n",
        "#%%\n",
        "#predList[\"Category\"]=predList[\"id\"]\n",
        "preds.rename(columns={\"id\": \"Category\"},inplace = True)\n",
        "\n",
        "#%%\n",
        "preds.drop(columns=[\"Sk\",\"count\",\"name\"], axis=1, inplace = True)\n",
        "#%%\n",
        "preds.sort_values(\"index1\", axis = 0, ascending = True, inplace = True, na_position ='last')\n",
        "#%%\n",
        "#predList=predList.drop(columns=[\"0_x\",\"0_y\"], axis=1)\n",
        "#%%\n",
        "##predList = (pd.merge(predList, , on=\"Sk\"))\n",
        "preds[\"file_name\"]=testFiles\n",
        "#%%\n",
        "data = readJson.readJSONFile(jsonTestFilePath)\n",
        "    \n",
        "images=data[\"images\"]\n",
        "\n",
        "images = pd.DataFrame.from_dict(images)\n",
        "#%%\n",
        "images = images[[\"id\",\"file_name\"]]\n",
        "# categories = pd.DataFrame.from_dict(categories)\n",
        "#%%\n",
        "preds=(pd.merge(preds, images, on=\"file_name\"))[[\"id\",\"Category\"]]\n",
        "\n",
        "#%%\n",
        "submission=pd.read_csv(\"../data/iwildcam-2020/sample_submission.csv\").drop(columns=[\"Category\"],axis=1)\n",
        "#%%\n",
        "preds.rename(columns={\"id\": \"Id\"},inplace = True)\n",
        "\n",
        "#%%\n",
        "preds=(pd.merge(submission, preds, on=\"Id\")) #[[\"Id\",\"Category\"]]\n",
        "\n",
        "#%%\n",
        "preds.to_csv(\"submissison.20200321.2.csv\",index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}