{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import src.readJson as readJson\n",
        "\n",
        "\n",
        "#%%\n",
        "root='C:\\\\Users\\\\manoj\\\\PycharmProjects\\\\tf-tuto\\\\data\\\\iwildcam-2020\\\\'\n",
        "jsonTrainFilePath=root+'iwildcam2020_train_annotations.json'\n",
        "jsonTestFilePath=root+'iwildcam2020_test_information.json'\n",
        "trainPath=root+'train\\\\28X28\\\\'\n",
        "testPath =root+'test\\\\100X100\\\\'\n",
        "#%%\n",
        "\n",
        "def getImageAsArray(path,df):\n",
        "    from PIL import Image\n",
        "    import os\n",
        "    dataF=[{\"file_name\":file_name,\n",
        "              \"image\":\n",
        "                  np.concatenate(\n",
        "                  (np.asarray(Image.open(os.path.join(path, file_name)).convert('L'))) #.tolist()\n",
        "                  ).ravel().tolist()\n",
        "              } \n",
        "             for file_name in df.file_name[:]]\n",
        "    return dataF\n",
        "#%%\n",
        "def getTrainData(jsonFilePath):\n",
        "    global categories\n",
        "    global trainDf\n",
        "    global trainDf1\n",
        "    data = readJson.readJSONFile(jsonFilePath)\n",
        "\n",
        "    annotations = data[\"annotations\"]\n",
        "    images=data[\"images\"]\n",
        "    categories = data[\"categories\"]\n",
        "    info = data[\"info\"]\n",
        "\n",
        "    # Convert to Data frame\n",
        "\n",
        "    annotations = pd.DataFrame.from_dict(annotations)\n",
        "    images = pd.DataFrame.from_dict(images)\n",
        "    categories = pd.DataFrame.from_dict(categories)\n",
        "\n",
        "    #Remove data from memory\n",
        "    del data\n",
        "\n",
        "    #Create column image_id to use for merging the two data frames\n",
        "    images[\"image_id\"]  = images[\"id\"]\n",
        "\n",
        "    # Merge annotations and images on image_id\n",
        "\n",
        "    trainDf1 = (pd.merge(annotations, images, on='image_id'))\n",
        "    #Remove Unnecessary fields\n",
        "    trainDf1.drop([\"id_y\",\"id_x\"], axis = 1, inplace=True)\n",
        "\n",
        "    #print(trainDf1.columns)\n",
        "    \n",
        "    # Unset annotations and images dataframe as they are no longer needed\n",
        "    del annotations\n",
        "    del images\n",
        "\n",
        "    # Open the image from working directory\n",
        "\n",
        "    trainDf=getImageAsArray(trainPath,trainDf1)\n",
        "\n",
        "    #Convert to dataframe\n",
        "    \n",
        "    trainDf = pd.DataFrame.from_dict(trainDf)\n",
        "\n",
        "\n",
        "    trainDf1.drop(['count', 'image_id', 'seq_id', 'width', 'height', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
        "\n",
        "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
        "    trainDf = (pd.merge(trainDf, trainDf1, on='file_name'))\n",
        "    \n",
        "    #del trainDf1\n",
        "\n",
        "    #trainDf=trainDf.drop(['file_name', 'seq_num_frames', 'location','datetime', 'frame_num'],axis=1)\n",
        "    #trainDf.drop(['seq_num_frames', 'location','datetime', 'frame_num'],axis=1, inplace=True)\n",
        "\n",
        "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
        "    trainDf=np.asarray(trainDf)\n",
        "\n",
        "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
        "    trainDf=[[arr[0]]+[arr[2]]+list(arr[1]) for arr in trainDf]\n",
        "    \n",
        "    # Converting back to dataframe\n",
        "    trainDf=pd.DataFrame(trainDf)\n",
        "    \n",
        "    #Write the processed file to csv file for future use\n",
        "    #trainDf.to_csv(\"trainDf.csv\")\n",
        "    \n",
        "    \n",
        "    categoriesFromTrainData=trainDf[1].unique()\n",
        "\n",
        "    categoriesFromTrainData=pd.DataFrame(categoriesFromTrainData)\n",
        "    categoriesFromTrainData.rename(columns={0: \"id\"},inplace = True)\n",
        "    categoriesFromTrainData.sort_values(\"id\", axis = 0, ascending = True, \n",
        "                  inplace = True, na_position ='last')\n",
        "    \n",
        "    categoriesFromTrainData[\"Sk\"]=[i for i in range(0,len(categoriesFromTrainData))]\n",
        "    \n",
        "    categories = (pd.merge(categoriesFromTrainData, categories, on='id'))\n",
        "    trainDf.rename(columns={0: \"file_name\",1:\"id\"},inplace = True)\n",
        "    trainDf = (pd.merge(trainDf, categories, on='id'))\n",
        "    trainDf[\"id\"]=trainDf[\"Sk\"]\n",
        "    trainDf.drop(columns=[\"Sk\",\"count\",\"name\",\"file_name\"],axis=1, inplace = True)\n",
        "    trainDf.rename(columns={\"id\": \"Sk\"},inplace = True)\n",
        "    # file_name=trainDf[[0,1]]\n",
        "    # file_name.rename(columns={0: \"file_name\",1:\"id\"},inplace = True)\n",
        "    # file_name = (pd.merge(file_name, categories, on='id'))\n",
        "    # file_name.sort_values(\"file_name\", axis = 0, ascending = True, \n",
        "    #                   inplace = True, na_position ='last')\n",
        "    # #%%\n",
        "    # \n",
        "    # trainDf.sort_values(\"file_name\", axis = 0, ascending = True, inplace = True, na_position ='last')\n",
        "    # #%%\n",
        "    \n",
        "    # def getSk(i):\n",
        "    #    return categories[categories[\"id\"]==i][\"Sk\"]\n",
        "    # #%%\n",
        "    # trainDf[\"Sk\"]=getSk(trainDf[\"id\"])\n",
        "\n",
        "    \n",
        "    #return categories, trainDf\n",
        "\n",
        "#%%\n",
        "def getTestData(jsonFilePath):\n",
        "    data = readJson.readJSONFile(jsonTestFilePath)\n",
        "    \n",
        "    images=data[\"images\"]\n",
        "    categories = data[\"categories\"]\n",
        "    info = data[\"info\"]\n",
        "    \n",
        "    # Convert to Data frame\n",
        "    \n",
        "    images = pd.DataFrame.from_dict(images)\n",
        "    categories = pd.DataFrame.from_dict(categories)\n",
        "    \n",
        "    #Remove data from memory\n",
        "    del data, info\n",
        "    \n",
        "    # Remove Unnecessary fields from images\n",
        "    testDf1 = pd.DataFrame(images.file_name)\n",
        "    \n",
        "    # Unset images dataframe as it is no longer needed\n",
        "    #del images\n",
        "    \n",
        "    # Open the image from working directory\n",
        "    \n",
        "    testDf=getImageAsArray(testPath,testDf1)\n",
        "    \n",
        "    #Convert to dataframe\n",
        "    \n",
        "    testDf = pd.DataFrame.from_dict(testDf)\n",
        "    \n",
        "    \n",
        "    # Merge the two training data frames to make sure the order is right and the image data is tied to file_name\n",
        "    testDf = (pd.merge(testDf, testDf1, on='file_name'))\n",
        "    \n",
        "    testFiles=testDf[\"file_name\"]\n",
        "    testDf.drop(['file_name'],axis=1, inplace=True)\n",
        "\n",
        "    # convert the dataframe to array so that it can be flattened, normal flatten() did not work\n",
        "    testDf=np.asarray(testDf)\n",
        "    \n",
        "    \n",
        "    # Concatenating the category_id and the image array of size=(100,100), this will flatten the data completely\n",
        "    testDf=[list(arr[0]) for arr in testDf]\n",
        "    \n",
        "    # Converting back to dataframe\n",
        "    testDf=pd.DataFrame(testDf)\n",
        "    \n",
        "    #Write the processed file to csv file for future use\n",
        "    #testDf.to_csv(\"testDf.100X100.csv\")\n",
        "    \n",
        "    return testFiles, testDf\n",
        "# In[1]:\n",
        "#categories, trainDf=getTrainData(jsonTrainFilePath)\n",
        "getTrainData(jsonTrainFilePath)\n",
        "\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#.insert (2, \"Sk\", file_name[\"Sk\"])\n",
        "#%%\n",
        "#trainDf.drop(columns=[0], axis=1)\n",
        "\n",
        "\n",
        "#trainDf=pd.read_csv(\"trainDf.100000.csv\")\n",
        "#%%\n",
        "#trainDf=trainDf.drop(columns=['Unnamed: 0'],axis=1)\n",
        "#%%\n",
        "# categories=trainDf[0].unique()\n",
        "\n",
        "# categories=pd.DataFrame(categories)\n",
        "\n",
        "# categories[\"Sk\"]=[i for i in range(0,len(categories))]\n",
        "# #%%\n",
        "# categories[\"id\"]=categories[0]\n",
        "# #%%\n",
        "# def getSk(i):\n",
        "#    return categories[categories[\"a\"]==i][\"b\"]\n",
        "\n",
        "# #%%\n",
        "# print(getSk(4))\n",
        "#%%\n",
        "#traindf[0]=[getSk(i) for i in trainDf[0]]\n",
        "# trainDf[\"id\"]=trainDf[0]\n",
        "\n",
        "# #%%\n",
        "# trainDf = (pd.merge(trainDf, categories, on=\"id\"))\n",
        "\n",
        "# #%%\n",
        "\n",
        "# trainDf = trainDf.drop(columns=[\"0_x\",\"0_y\",\"id\"], axis=1)\n",
        "#%%\n",
        "#trainDf.to_csv(\"train.csv\")\n",
        "trainDf=pd.read_csv(\"train.csv\")\n",
        "#.to_csv(\"train.csv\")\n",
        "\n",
        "# In[1]:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"0\"]), trainDf[\"0\"], test_size=0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(trainDf.drop(columns=[\"Sk\"]), trainDf[\"Sk\"], test_size=0.1)\n",
        "#%%\n",
        "#Skipping Test train Split\n",
        "#X_train=trainDf.drop(columns=[\"Sk\"])\n",
        "X_train=trainDf.drop(columns=[\"Sk\",\"Unnamed: 0\"])\n",
        "y_train=trainDf[\"Sk\"]\n",
        "#%%\n",
        "del trainDf\n",
        "# In[2]:\n",
        "\n",
        "X_train = X_train/255.0\n",
        "#X_test  = X_test /255.0\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "X_train.replace(0,1e-10, inplace=True)\n",
        "#%%\n",
        "X_test =X_test.replace(0,1e-10)\n",
        "#%%\n",
        "# Replace very small number with zero  \n",
        "#X_train[X_train == 1e-10] = 0\n",
        "\n",
        "#%%\n",
        "#X_train =X_train.replace(1e-10,0)\n",
        "#%%\n",
        "'''\n",
        "\n",
        "X_train[X_train == 0] = 1e-10\n",
        "X_test [X_test  == 0] = 1e-10\n",
        "\n",
        "'''\n",
        "\n",
        "# In[4]:\n",
        "X_train = X_train.values.reshape(-1,100,100,1)\n",
        "#X_test = X_test.values.reshape(-1,100,100,1)\n",
        "# In[5]:\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "X_train = np.asarray(X_train)\n",
        "# In[6]:\n",
        "\n",
        "print(X_train[np.isnan(X_train)])\n",
        "print(X_test [np.isnan(X_test )])\n",
        "print(y_train[np.isnan(y_train)])\n",
        "print(y_test [np.isnan(y_test )])\n",
        "\n",
        "#%%\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "#%%\n",
        "#model=tf.keras.models.load_model(\"model.1.save\")\n",
        "#%%\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (100,100,1)))\n",
        "model.add(keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "# model.add(keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "# model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "# model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(1024, activation = \"relu\"))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "#model.add(keras.layers.Dense(267, activation = \"softmax\"))\n",
        "model.add(keras.layers.Dense(len(categories), activation = \"softmax\"))\n",
        "\n",
        "#model.add(keras.layers.Dense(len(categories), activation = \"softmax\", weights = [np.zeros([[100,100,1], len(categories)]), np.zeros(len(categories))]))\n",
        "\n",
        "#optimizer = keras.optimizers.RMSprop(learning_rate=0.00, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "#optimizer = keras.optimizers.SGD()\n",
        "#optimizer = keras.optimizers.RMSprop()\n",
        "\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "model.load_weights(filepath='src/final_weight.h5')\n",
        "#%%\n",
        "model.fit(X_train, y_train, epochs=8)\n",
        "#%%\n",
        "model.save_weights(filepath='final_weight.conv2x3.1024.h5')\n",
        "#%%\n",
        "testFiles, testDf =getTestData(jsonTestFilePath)\n",
        "\n",
        "#%%\n",
        "testDf.to_csv(\"testDf.csv\")\n",
        "#%%\n",
        "pd.DataFrame(testFiles).to_csv(\"testFiles.csv\")\n",
        "# In[2]:\n",
        "# Normalize the values\n",
        "testDf = testDf /255.0\n",
        "\n",
        "#%%\n",
        "\n",
        "X_train.replace(0,1e-10, inplace=True)\n",
        "\n",
        "#%%\n",
        "\n",
        "#testDf = testDf.values.reshape(-1,100,100,1)\n",
        "\n",
        "#%%\n",
        "# Replace very small number with zero  \n",
        "testDf[testDf == 0] = 1e-10\n",
        "\n",
        "#%%\n",
        "preds = model.predict(testDf)\n",
        "#%%\n",
        "#%%\n",
        "preds = pd.DataFrame(data=preds, columns=range(0,len(categories)))\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "preds = preds.idxmax(axis=1)\n",
        "\n",
        "#%%\n",
        "preds=pd.DataFrame(preds)\n",
        "#%%\n",
        "\n",
        "preds['index1'] = preds.index\n",
        "\n",
        "#%%\n",
        "\n",
        "#predList[\"Sk\"]=predList[0]\n",
        "preds.rename(columns={0: \"Sk\"},inplace = True)\n",
        "#%%\n",
        "#predList=predList.drop(columns=[0],axis=1)\n",
        "#%%\n",
        "\n",
        "preds = (pd.merge(preds, categories, on=\"Sk\"))\n",
        "\n",
        "#%%\n",
        "#.drop(columns=[\"\",\"\"])\n",
        "#%%\n",
        "#del X_train\n",
        "\n",
        "#%%\n",
        "#predList[\"Category\"]=predList[\"id\"]\n",
        "preds.rename(columns={\"id\": \"Category\"},inplace = True)\n",
        "\n",
        "#%%\n",
        "preds.drop(columns=[\"Sk\",\"count\",\"name\"], axis=1, inplace = True)\n",
        "#%%\n",
        "preds.sort_values(\"index1\", axis = 0, ascending = True, inplace = True, na_position ='last')\n",
        "#%%\n",
        "#predList=predList.drop(columns=[\"0_x\",\"0_y\"], axis=1)\n",
        "#%%\n",
        "##predList = (pd.merge(predList, , on=\"Sk\"))\n",
        "preds[\"file_name\"]=testFiles\n",
        "#%%\n",
        "data = readJson.readJSONFile(jsonTestFilePath)\n",
        "    \n",
        "images=data[\"images\"]\n",
        "\n",
        "images = pd.DataFrame.from_dict(images)\n",
        "#%%\n",
        "images = images[[\"id\",\"file_name\"]]\n",
        "# categories = pd.DataFrame.from_dict(categories)\n",
        "#%%\n",
        "preds=(pd.merge(preds, images, on=\"file_name\"))[[\"id\",\"Category\"]]\n",
        "\n",
        "#%%\n",
        "submission=pd.read_csv(\"../data/iwildcam-2020/sample_submission.csv\").drop(columns=[\"Category\"],axis=1)\n",
        "#%%\n",
        "preds.rename(columns={\"id\": \"Id\"},inplace = True)\n",
        "\n",
        "#%%\n",
        "preds=(pd.merge(submission, preds, on=\"Id\")) #[[\"Id\",\"Category\"]]\n",
        "\n",
        "#%%\n",
        "preds.to_csv(\"submissison.20200321.2.csv\",index=False)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}